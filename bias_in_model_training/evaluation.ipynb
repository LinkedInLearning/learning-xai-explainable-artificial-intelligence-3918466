{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI Output Evaluation Pipeline\n",
    "## Overview\n",
    "This document provides both documentation and complete implementation of a comprehensive pipeline for evaluating Generative AI outputs across multiple dimensions including bias, fairness, and quality. The pipeline is designed for educational purposes, helping students understand how to systematically assess AI-generated content.\n",
    "\n",
    "\n",
    "## Installation\n",
    "### Prerequisites\n",
    "Ensure you have Python 3.8+ installed. The project dependencies are listed in requirements.txt:\n",
    "```bash \n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Next, download the correct spaCy model:\n",
    "```bash\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generative AI Evaluation Pipeline\n",
    "\n",
    "This module implements a comprehensive evaluation pipeline for assessing Generative AI outputs\n",
    "across multiple dimensions including bias, fairness, and quality.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import textstat\n",
    "from transformers import pipeline\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class AIOutputEvaluator:\n",
    "    \"\"\"\n",
    "    A comprehensive pipeline for evaluating Generative AI outputs across multiple dimensions.\n",
    "    \n",
    "    This class implements various evaluation metrics and provides detailed feedback suitable\n",
    "    for educational purposes. Students can learn about different aspects of AI evaluation\n",
    "    while getting hands-on experience with industry-standard tools.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with necessary models and resources.\n",
    "        \"\"\"\n",
    "        # Load spaCy model for text processing\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Initialize sentiment analyzer\n",
    "        self.sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "        \n",
    "        # Define protected group terms for fairness evaluation\n",
    "        self.protected_groups = {\n",
    "            \"gender\": [\"he\", \"she\", \"they\"],\n",
    "            \"ethnicity\": [\"Asian\", \"Black\", \"Hispanic\", \"White\"],\n",
    "            \"age\": [\"young\", \"elderly\", \"middle-aged\"]\n",
    "        }\n",
    "        \n",
    "    def evaluate_bias(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate potential biases in the generated text.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): The AI-generated text to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing bias metrics\n",
    "            \n",
    "        Student Note:\n",
    "            This function helps identify potential biases by analyzing:\n",
    "            1. Representation bias: How different groups are represented\n",
    "            2. Language bias: Use of stereotypical or prejudiced language\n",
    "            3. Association bias: Implicit associations between concepts\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        bias_metrics = {}\n",
    "        \n",
    "        # Analyze representation across protected groups\n",
    "        group_mentions = defaultdict(int)\n",
    "        for token in doc:\n",
    "            for category, terms in self.protected_groups.items():\n",
    "                if token.text.lower() in [t.lower() for t in terms]:\n",
    "                    group_mentions[category] += 1\n",
    "        \n",
    "        # Calculate representation disparity\n",
    "        for category, counts in group_mentions.items():\n",
    "            if counts > 0:\n",
    "                max_mentions = max(counts)\n",
    "                min_mentions = min(counts)\n",
    "                bias_metrics[f\"{category}_disparity\"] = max_mentions / (min_mentions + 1)\n",
    "        \n",
    "        # Analyze sentiment associations\n",
    "        for category, terms in self.protected_groups.items():\n",
    "            category_sentiments = []\n",
    "            for term in terms:\n",
    "                # Find sentences containing the term\n",
    "                relevant_sents = [sent for sent in sent_tokenize(text) \n",
    "                                if term.lower() in sent.lower()]\n",
    "                if relevant_sents:\n",
    "                    sentiments = [self.sentiment_analyzer(sent)[0]['score'] \n",
    "                                for sent in relevant_sents]\n",
    "                    category_sentiments.extend(sentiments)\n",
    "            \n",
    "            if category_sentiments:\n",
    "                bias_metrics[f\"{category}_sentiment_std\"] = np.std(category_sentiments)\n",
    "        \n",
    "        return bias_metrics\n",
    "    \n",
    "    def evaluate_fairness(self, outputs: List[str], \n",
    "                         demographic_inputs: List[str],\n",
    "                         ground_truth: List[bool] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate fairness of the AI system across different demographic groups.\n",
    "        \n",
    "        Parameters:\n",
    "            outputs (List[str]): List of AI-generated outputs\n",
    "            demographic_inputs (List[str]): Corresponding demographic information\n",
    "            ground_truth (List[bool], optional): True labels for equalized odds calculation\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing fairness metrics\n",
    "            \n",
    "        Student Note:\n",
    "            This function implements three key fairness metrics:\n",
    "            1. Demographic Parity: Ensures the model makes positive predictions at equal \n",
    "               rates across different demographic groups\n",
    "            2. Equalized Odds Parity: Ensures equal true positive and false positive \n",
    "               rates across groups\n",
    "            3. Equal Opportunity Parity: Ensures equal true positive rates across groups\n",
    "        \"\"\"\n",
    "        fairness_metrics = {}\n",
    "        \n",
    "        # Calculate quality scores and convert to binary decisions\n",
    "        group_decisions = defaultdict(list)\n",
    "        group_scores = defaultdict(list)\n",
    "        threshold = 0.5  # Quality threshold for binary decision\n",
    "        \n",
    "        for output, demo in zip(outputs, demographic_inputs):\n",
    "            quality_score = self._calculate_quality_score(output)\n",
    "            binary_decision = quality_score >= threshold\n",
    "            group_decisions[demo].append(binary_decision)\n",
    "            group_scores[demo].append(quality_score)\n",
    "        \n",
    "        # Calculate Demographic Parity\n",
    "        group_acceptance_rates = {}\n",
    "        for group, decisions in group_decisions.items():\n",
    "            acceptance_rate = np.mean(decisions)\n",
    "            group_acceptance_rates[group] = acceptance_rate\n",
    "            fairness_metrics[f\"{group}_acceptance_rate\"] = acceptance_rate\n",
    "        \n",
    "        if len(group_acceptance_rates) > 1:\n",
    "            fairness_metrics[\"demographic_parity_diff\"] = (\n",
    "                max(group_acceptance_rates.values()) - min(group_acceptance_rates.values())\n",
    "            )\n",
    "        \n",
    "        # Calculate Equalized Odds and Equal Opportunity if ground truth is provided\n",
    "        if ground_truth is not None:\n",
    "            group_metrics = defaultdict(lambda: {\"TP\": 0, \"FP\": 0, \"TN\": 0, \"FN\": 0})\n",
    "            \n",
    "            for demo, pred, true in zip(demographic_inputs, \n",
    "                                      [score >= threshold for group in group_scores.values() \n",
    "                                       for score in group], \n",
    "                                      ground_truth):\n",
    "                if true:\n",
    "                    if pred:\n",
    "                        group_metrics[demo][\"TP\"] += 1\n",
    "                    else:\n",
    "                        group_metrics[demo][\"FN\"] += 1\n",
    "                else:\n",
    "                    if pred:\n",
    "                        group_metrics[demo][\"FP\"] += 1\n",
    "                    else:\n",
    "                        group_metrics[demo][\"TN\"] += 1\n",
    "            \n",
    "            # Calculate TPR and FPR for each group\n",
    "            for group, metrics in group_metrics.items():\n",
    "                # True Positive Rate (TPR)\n",
    "                tpr = (metrics[\"TP\"] / (metrics[\"TP\"] + metrics[\"FN\"]) \n",
    "                      if metrics[\"TP\"] + metrics[\"FN\"] > 0 else 0)\n",
    "                # False Positive Rate (FPR)\n",
    "                fpr = (metrics[\"FP\"] / (metrics[\"FP\"] + metrics[\"TN\"]) \n",
    "                      if metrics[\"FP\"] + metrics[\"TN\"] > 0 else 0)\n",
    "                \n",
    "                fairness_metrics[f\"{group}_TPR\"] = tpr\n",
    "                fairness_metrics[f\"{group}_FPR\"] = fpr\n",
    "            \n",
    "            # Calculate Equalized Odds difference\n",
    "            if len(group_metrics) > 1:\n",
    "                tpr_diff = (max(fairness_metrics[f\"{g}_TPR\"] \n",
    "                              for g in group_metrics.keys()) -\n",
    "                          min(fairness_metrics[f\"{g}_TPR\"] \n",
    "                              for g in group_metrics.keys()))\n",
    "                fpr_diff = (max(fairness_metrics[f\"{g}_FPR\"] \n",
    "                              for g in group_metrics.keys()) -\n",
    "                          min(fairness_metrics[f\"{g}_FPR\"] \n",
    "                              for g in group_metrics.keys()))\n",
    "                \n",
    "                fairness_metrics[\"equalized_odds_diff\"] = (tpr_diff + fpr_diff) / 2\n",
    "                fairness_metrics[\"equal_opportunity_diff\"] = tpr_diff\n",
    "        \n",
    "        # Calculate general fairness metrics\n",
    "        for group, scores in group_scores.items():\n",
    "            fairness_metrics[f\"{group}_mean\"] = np.mean(scores)\n",
    "            fairness_metrics[f\"{group}_std\"] = np.std(scores)\n",
    "        \n",
    "        # Calculate overall disparity\n",
    "        if len(group_scores) > 1:\n",
    "            group_means = [np.mean(scores) for scores in group_scores.values()]\n",
    "            fairness_metrics[\"max_disparity\"] = max(group_means) / min(group_means)\n",
    "        \n",
    "        return fairness_metrics\n",
    "    \n",
    "    def evaluate_quality(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the general quality of the generated text.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): The AI-generated text to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing quality metrics\n",
    "            \n",
    "        Student Note:\n",
    "            Quality evaluation examines multiple aspects of the text:\n",
    "            1. Readability: How easy is it to understand?\n",
    "            2. Coherence: Does it make logical sense?\n",
    "            3. Fluency: Is it well-written and natural?\n",
    "            4. Relevance: Does it address the intended topic?\n",
    "        \"\"\"\n",
    "        quality_metrics = {}\n",
    "        \n",
    "        # Readability metrics\n",
    "        quality_metrics[\"flesch_reading_ease\"] = textstat.flesch_reading_ease(text)\n",
    "        quality_metrics[\"gunning_fog\"] = textstat.gunning_fog(text)\n",
    "        \n",
    "        # Coherence metrics\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        if len(sentences) > 1:\n",
    "            coherence_scores = []\n",
    "            for i in range(len(sentences) - 1):\n",
    "                similarity = sentences[i].similarity(sentences[i + 1])\n",
    "                coherence_scores.append(similarity)\n",
    "            quality_metrics[\"coherence\"] = np.mean(coherence_scores)\n",
    "        \n",
    "        # Fluency metrics\n",
    "        quality_metrics[\"avg_sentence_length\"] = len(text.split()) / len(sentences)\n",
    "        quality_metrics[\"vocabulary_diversity\"] = len(set(text.split())) / len(text.split())\n",
    "        \n",
    "        return quality_metrics\n",
    "    \n",
    "    def _calculate_quality_score(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate an overall quality score for fairness comparison.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): The text to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            float: Combined quality score\n",
    "        \"\"\"\n",
    "        quality_metrics = self.evaluate_quality(text)\n",
    "        # Normalize and combine metrics\n",
    "        score = (quality_metrics.get(\"flesch_reading_ease\", 0) / 100 +\n",
    "                (1 - quality_metrics.get(\"gunning_fog\", 0) / 20) +\n",
    "                quality_metrics.get(\"coherence\", 0) +\n",
    "                quality_metrics.get(\"vocabulary_diversity\", 0)) / 4\n",
    "        return score\n",
    "    \n",
    "    def generate_report(self, text: str, demographic_info: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive evaluation report.\n",
    "        \n",
    "        Parameters:\n",
    "            text (str): The AI-generated text to evaluate\n",
    "            demographic_info (str, optional): Demographic context if available\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Complete evaluation report with all metrics\n",
    "            \n",
    "        Student Note:\n",
    "            This function combines all evaluation aspects into a single report.\n",
    "            Use this to:\n",
    "            1. Get a complete picture of the AI output's performance\n",
    "            2. Identify areas needing improvement\n",
    "            3. Compare performance across different contexts\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            \"bias_metrics\": self.evaluate_bias(text),\n",
    "            \"quality_metrics\": self.evaluate_quality(text)\n",
    "        }\n",
    "        \n",
    "        if demographic_info:\n",
    "            report[\"fairness_metrics\"] = self.evaluate_fairness([text], [demographic_info])\n",
    "        \n",
    "        # Add summary statistics\n",
    "        report[\"summary\"] = {\n",
    "            \"overall_quality\": np.mean(list(report[\"quality_metrics\"].values())),\n",
    "            \"bias_level\": np.mean(list(report[\"bias_metrics\"].values())),\n",
    "            \"timestamp\": pd.Timestamp.now()\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def visualize_results(self, report: Dict[str, Any]) -> None:\n",
    "        \"\"\"\n",
    "        Create visualizations of evaluation results.\n",
    "        \n",
    "        Parameters:\n",
    "            report (Dict[str, Any]): Evaluation report to visualize\n",
    "            \n",
    "        Student Note:\n",
    "            Visualization helps in:\n",
    "            1. Understanding patterns in the data\n",
    "            2. Identifying potential issues quickly\n",
    "            3. Communicating results effectively\n",
    "        \"\"\"\n",
    "        # Create a figure with multiple subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot bias metrics\n",
    "        bias_data = pd.Series(report[\"bias_metrics\"])\n",
    "        sns.barplot(x=bias_data.index, y=bias_data.values, ax=axes[0, 0])\n",
    "        axes[0, 0].set_title(\"Bias Metrics\")\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot quality metrics\n",
    "        quality_data = pd.Series(report[\"quality_metrics\"])\n",
    "        sns.barplot(x=quality_data.index, y=quality_data.values, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"Quality Metrics\")\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot fairness metrics if available\n",
    "        if \"fairness_metrics\" in report:\n",
    "            fairness_data = pd.Series(report[\"fairness_metrics\"])\n",
    "            sns.barplot(x=fairness_data.index, y=fairness_data.values, ax=axes[1, 0])\n",
    "            axes[1, 0].set_title(\"Fairness Metrics\")\n",
    "            axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot summary metrics\n",
    "        summary_data = pd.Series(report[\"summary\"])\n",
    "        summary_data = summary_data.drop('timestamp')  # Remove non-numeric data\n",
    "        sns.barplot(x=summary_data.index, y=summary_data.values, ax=axes[1, 1])\n",
    "        axes[1, 1].set_title(\"Summary Metrics\")\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "Here's how to use the evaluation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = AIOutputEvaluator()\n",
    "\n",
    "# Example text to evaluate\n",
    "sample_text = \"\"\"\n",
    "The young software engineer presented her innovative solution to the team.\n",
    "Her colleagues, both young and experienced, were impressed by the elegant design.\n",
    "The project manager decided to implement the solution across all departments.\n",
    "\"\"\"\n",
    "\n",
    "# Generate comprehensive report\n",
    "report = evaluator.generate_report(sample_text)\n",
    "\n",
    "# Visualize results\n",
    "evaluator.visualize_results(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic Parity - Measures whether the model's positive predictions are equal across demographic groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Resume Summary Generation\n",
    "texts = [\n",
    "    \"Experienced software engineer with 10 years at tech companies, led multiple teams.\",\n",
    "    \"Recent computer science graduate with internship experience in software development.\",\n",
    "    \"Mid-level developer with 5 years experience in full-stack development.\",\n",
    "    \"Senior engineer with extensive backend experience and team leadership.\",\n",
    "]\n",
    "demographics = [\"older\", \"younger\", \"younger\", \"older\"]\n",
    "\n",
    "# Evaluate if the model rates resumes fairly regardless of age\n",
    "report = evaluator.evaluate_fairness(texts, demographics)\n",
    "print(f\"Demographic Parity Difference: {report['demographic_parity_diff']}\")\n",
    "# A high difference would indicate the model systematically rates one age group's \n",
    "# resumes higher than the other's, regardless of content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equalized Odds Parity - Ensures equal true positive and false positive rates across groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Medical Diagnosis Summary Generation\n",
    "texts = [\n",
    "    \"Patient presents with severe symptoms requiring immediate intervention.\",\n",
    "    \"Minor symptoms observed, recommend routine follow-up.\",\n",
    "    \"Moderate symptoms indicate need for additional testing.\",\n",
    "    \"Critical symptoms detected, emergency care recommended.\"\n",
    "]\n",
    "demographics = [\"group_a\", \"group_b\", \"group_a\", \"group_b\"]\n",
    "# Ground truth from actual medical outcomes\n",
    "ground_truth = [True, False, True, True]  # True = needed intervention\n",
    "\n",
    "report = evaluator.evaluate_fairness(texts, demographics, ground_truth)\n",
    "print(f\"Equalized Odds Difference: {report['equalized_odds_diff']}\")\n",
    "# A high difference would indicate the model is better at identifying \n",
    "# medical needs for one demographic group over another"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal Opportunity Parity - Ensures equal true positive rates across groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Job Application Response Generation\n",
    "texts = [\n",
    "    \"Candidate meets all required qualifications with relevant experience.\",\n",
    "    \"Application shows strong potential but lacks specific requirements.\",\n",
    "    \"Highly qualified candidate with extensive relevant background.\",\n",
    "    \"Promising candidate with transferable skills from different field.\"\n",
    "]\n",
    "demographics = [\"majority\", \"minority\", \"minority\", \"majority\"]\n",
    "# Ground truth from actual hiring outcomes\n",
    "ground_truth = [True, False, True, False]  # True = was qualified\n",
    "\n",
    "report = evaluator.evaluate_fairness(texts, demographics, ground_truth)\n",
    "print(f\"Equal Opportunity Difference: {report['equal_opportunity_diff']}\")\n",
    "# A high difference would indicate the model is better at identifying\n",
    "# qualified candidates from one demographic group over another"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Results:\n",
    "\n",
    "## Demographic Parity Difference\n",
    "\n",
    "0.0 = Perfect parity (same rate of positive predictions across groups)\n",
    "Higher values indicate greater disparity in prediction rates\n",
    "Example: 0.2 means one group gets 20% more positive predictions\n",
    "\n",
    "\n",
    "## Equalized Odds Difference\n",
    "\n",
    "0.0 = Perfect parity (same TPR and FPR across groups)\n",
    "Higher values indicate the model's accuracy varies by group\n",
    "Example: 0.15 means 15% difference in error rates between groups\n",
    "\n",
    "\n",
    "## Equal Opportunity Difference\n",
    "\n",
    "0.0 = Perfect parity (same true positive rate across groups)\n",
    "Higher values indicate bias in identifying positive cases\n",
    "Example: 0.25 means 25% difference in true positive rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps to Consider\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Clean and normalize input text\n",
    "Handle edge cases (empty strings, very long texts)\n",
    "Validate demographic information\n",
    "\n",
    "\n",
    "## Evaluation Strategy\n",
    "\n",
    "Use multiple metrics for each dimension\n",
    "Consider context-specific requirements\n",
    "Document assumptions and limitations\n",
    "\n",
    "\n",
    "## Result Interpretation\n",
    "\n",
    "Consider relative scores rather than absolute values\n",
    "Look for patterns across multiple outputs\n",
    "Account for domain-specific considerations"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
