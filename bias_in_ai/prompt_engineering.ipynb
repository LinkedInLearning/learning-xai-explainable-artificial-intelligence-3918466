{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering: A Structured Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This Jupyter notebook provides a structured approach to prompt engineering using Ollama, focusing on generating clear, neutral, and inclusive content. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- Jupyter Notebook/Lab environment\n",
    "- Ollama installed on your system\n",
    "- Basic understanding of Python and API interactions\n",
    "\n",
    "## Setting Up Ollama\n",
    "\n",
    "Install Ollama by following the instructions at [Ollama's official website](https://ollama.com/)\n",
    "and following the instructions from ollama to download LLAMA 3.2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pull LLAMA 3 using bash**:\n",
    "```bash\n",
    "ollama pull llama3.3\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Environment Setup\n",
    "Create a new virtual environment and install the required packages in bash:\n",
    "```bash\n",
    "python -m venv ollama-env\n",
    "source ollama-env/bin/activate  # On Windows: ollama-env\\Scripts\\activate\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Ollama Prompt Engineering Module\n",
    "\n",
    "This module provides utilities for interacting with Ollama locally\n",
    "and implementing structured prompt engineering approaches.\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "class LocalPromptEngineer:\n",
    "    \"\"\"A class for managing prompt engineering with local Ollama installation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"llama2\"):\n",
    "        \"\"\"\n",
    "        Initialize the LocalPromptEngineer class.\n",
    "        \n",
    "        Args:\n",
    "            model (str): Name of the Ollama model to use\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.history: List[Dict[str, Any]] = []\n",
    "        \n",
    "        # Verify Ollama installation and model\n",
    "        try:\n",
    "            # Check if Ollama is installed\n",
    "            subprocess.run(['ollama', '--version'], check=True, capture_output=True)\n",
    "            \n",
    "            # Check if model exists\n",
    "            result = subprocess.run(['ollama', 'list'], check=True, capture_output=True, text=True)\n",
    "            available_models = result.stdout.lower()\n",
    "            \n",
    "            if self.model.lower() not in available_models:\n",
    "                raise RuntimeError(\n",
    "                    f\"Model '{self.model}' not found. Please run 'ollama pull {self.model}' first.\\n\"\n",
    "                    f\"Available models: {available_models}\"\n",
    "                )\n",
    "                \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise RuntimeError(f\"Error checking Ollama: {e.stderr}\") from e\n",
    "        except FileNotFoundError as e:\n",
    "            raise RuntimeError(\"Ollama command not found. Please install Ollama first.\") from e\n",
    "    \n",
    "    def create_structured_prompt(\n",
    "        self,\n",
    "        task: str,\n",
    "        context: str = \"\",\n",
    "        constraints: List[str] = None,\n",
    "        examples: List[Dict[str, str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Create a structured prompt following best practices.\n",
    "        \n",
    "        Args:\n",
    "            task (str): Main task description\n",
    "            context (str): Additional context for the task\n",
    "            constraints (List[str]): List of constraints to apply\n",
    "            examples (List[Dict[str, str]]): List of example input/output pairs\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted prompt\n",
    "        \"\"\"\n",
    "        constraints = constraints or []\n",
    "        examples = examples or []\n",
    "        \n",
    "        prompt_parts = [\n",
    "            \"# Task\",\n",
    "            task,\n",
    "            \"\\n# Context\" if context else \"\",\n",
    "            context if context else \"\",\n",
    "            \"\\n# Constraints:\" if constraints else \"\",\n",
    "            \"\\n\".join(f\"- {c}\" for c in constraints) if constraints else \"\",\n",
    "            \"\\n# Examples:\" if examples else \"\",\n",
    "        ]\n",
    "        \n",
    "        for example in examples:\n",
    "            prompt_parts.extend([\n",
    "                \"\\nInput:\",\n",
    "                example.get(\"input\", \"\"),\n",
    "                \"\\nOutput:\",\n",
    "                example.get(\"output\", \"\"),\n",
    "            ])\n",
    "            \n",
    "        return \"\\n\".join(filter(None, prompt_parts))\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        temperature: float = 0.7\n",
    "    ) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Generate a response using local Ollama installation.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The structured prompt\n",
    "            temperature (float): Creativity parameter (0.0 - 1.0)\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, str]: Response from the model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First test if the model can be reached\n",
    "            test_cmd = ['ollama', 'list']\n",
    "            subprocess.run(test_cmd, check=True, capture_output=True, text=True)\n",
    "            \n",
    "            # Escape the prompt for shell\n",
    "            escaped_prompt = prompt.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "            \n",
    "            # Construct the command without shell\n",
    "            cmd = [\n",
    "                'ollama', \n",
    "                'run',\n",
    "                self.model,\n",
    "                escaped_prompt\n",
    "            ]\n",
    "            \n",
    "            # Run the command without shell\n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                shell=False,  # Don't use shell to avoid escaping issues\n",
    "                capture_output=True,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                logging.error(f\"Command failed with error: {result.stderr}\")\n",
    "                return {\"response\": \"\", \"error\": result.stderr}\n",
    "            \n",
    "            response = {\n",
    "                \"response\": result.stdout.strip(),\n",
    "                \"error\": result.stderr.strip() if result.stderr else None\n",
    "            }\n",
    "            \n",
    "            # Log the interaction\n",
    "            self.history.append({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"model\": self.model,\n",
    "                \"temperature\": temperature\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logging.error(f\"Error generating response: {e}\")\n",
    "            logging.error(f\"Command output: {e.output}\")\n",
    "            return {\"response\": \"\", \"error\": str(e)}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            return {\"response\": \"\", \"error\": str(e)}\n",
    "    \n",
    "    def _evaluate_general_criterion(self, text: str, criterion: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate text for general criteria like clarity, technical accuracy, etc.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to evaluate\n",
    "            criterion (str): Criterion to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            float: Score for the criterion (0-1)\n",
    "        \"\"\"\n",
    "        criterion_patterns = {\n",
    "            'clarity': {\n",
    "                'positive': [\n",
    "                    r'\\b(clearly|specifically|precisely)\\b',\n",
    "                    r'\\bfor example\\b',\n",
    "                    r'\\bin other words\\b',\n",
    "                ],\n",
    "                'negative': [\n",
    "                    r'\\b(maybe|perhaps|possibly)\\b',\n",
    "                    r'\\b(complicated|complex)\\b(?!.*\\b(explained|simplified)\\b)',\n",
    "                ]\n",
    "            },\n",
    "            'technical_accuracy': {\n",
    "                'positive': [\n",
    "                    r'\\b(algorithm|function|method)\\b(?=.*\\b(implementation|example|usage)\\b)',\n",
    "                    r'\\b(input|output)\\b(?=.*\\b(example|parameter|argument)\\b)',\n",
    "                ],\n",
    "                'negative': [\n",
    "                    r'\\b(magic|somehow|mysteriously)\\b',\n",
    "                    r'\\b(always|never)\\b(?=.*\\b(works|fails)\\b)',\n",
    "                ]\n",
    "            },\n",
    "            'engagement': {\n",
    "                'positive': [\n",
    "                    r'\\b(imagine|consider|think about)\\b',\n",
    "                    r'\\b(real-world|practical|everyday)\\b',\n",
    "                    r'\\blet\\'s\\b',\n",
    "                ],\n",
    "                'negative': [\n",
    "                    r'\\b(boring|tedious|difficult)\\b',\n",
    "                    r'\\b(simply|just|easily)\\b(?=.*\\b(do|understand)\\b)',\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if criterion not in criterion_patterns:\n",
    "            return 0.5  # Default score for unknown criteria\n",
    "            \n",
    "        patterns = criterion_patterns[criterion]\n",
    "        score = 0.5  # Start with neutral score\n",
    "        \n",
    "        # Check positive patterns\n",
    "        for pattern in patterns['positive']:\n",
    "            matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "            score += matches * 0.1  # Increment score for each positive match\n",
    "            \n",
    "        # Check negative patterns\n",
    "        for pattern in patterns['negative']:\n",
    "            matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "            score -= matches * 0.1  # Decrement score for each negative match\n",
    "            \n",
    "        # Normalize score to 0-1 range\n",
    "        return max(0.0, min(1.0, score))\n",
    "\n",
    "    def _evaluate_bias(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate text for various types of bias.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            float: Bias score (0-1, where 0 is least biased)\n",
    "        \"\"\"\n",
    "        bias_indicators = {\n",
    "            'gender_bias': {\n",
    "                'patterns': [\n",
    "                    r'\\b(he|his|him|gentleman|man|men)\\b(?!.*\\b(she|her|hers|lady|woman|women)\\b)',\n",
    "                    r'\\b(she|her|hers|lady|woman|women)\\b(?!.*\\b(he|his|him|gentleman|man|men)\\b)',\n",
    "                    r'\\b(businessman|businesswoman|chairman|chairwoman|spokesman|spokeswoman)\\b'\n",
    "                ],\n",
    "                'weight': 0.3\n",
    "            },\n",
    "            'racial_bias': {\n",
    "                'patterns': [\n",
    "                    r'\\b(normal|standard|regular|typical|default)(?=\\s+(person|people|individual|community))\\b',\n",
    "                    r'\\b(ethnic|minority|diverse)(?=\\s+only\\b)',\n",
    "                ],\n",
    "                'weight': 0.3\n",
    "            },\n",
    "            'age_bias': {\n",
    "                'patterns': [\n",
    "                    r'\\b(young|old|elderly|senior)(?=\\s+people\\b)',\n",
    "                    r'\\b(millennials|boomers|gen\\s+[xyz])\\b\\s+(?=\\b(are|always|never|typically)\\b)',\n",
    "                ],\n",
    "                'weight': 0.2\n",
    "            },\n",
    "            'socioeconomic_bias': {\n",
    "                'patterns': [\n",
    "                    r'\\b(poor|rich|wealthy|low-income|high-income)(?=\\s+people\\b)',\n",
    "                    r'\\b(educated|uneducated|privileged|underprivileged)\\b',\n",
    "                ],\n",
    "                'weight': 0.2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        bias_score = 0.0\n",
    "        \n",
    "        for bias_type, config in bias_indicators.items():\n",
    "            type_score = 0\n",
    "            for pattern in config['patterns']:\n",
    "                matches = len(re.findall(pattern, text, re.IGNORECASE))\n",
    "                if matches > 0:\n",
    "                    type_score += matches * 0.1\n",
    "            \n",
    "            bias_score += type_score * config['weight']\n",
    "        \n",
    "        return min(1.0, bias_score)\n",
    "\n",
    "    def evaluate_response(\n",
    "        self,\n",
    "        response: Dict[str, str],\n",
    "        criteria: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the quality of a response based on given criteria.\n",
    "        \n",
    "        Args:\n",
    "            response (Dict[str, str]): Response from the model\n",
    "            criteria (List[str]): List of evaluation criteria\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, float]: Evaluation scores\n",
    "        \"\"\"\n",
    "        text = response.get(\"response\", \"\")\n",
    "        scores = {}\n",
    "        \n",
    "        for criterion in criteria:\n",
    "            if criterion == \"bias\":\n",
    "                scores[criterion] = self._evaluate_bias(text)\n",
    "            else:\n",
    "                scores[criterion] = self._evaluate_general_criterion(text, criterion)\n",
    "        \n",
    "        return scores\n",
    "\n",
    "    def save_history(self, filepath: Union[str, Path]) -> None:\n",
    "        \"\"\"\n",
    "        Save interaction history to a JSON file.\n",
    "        \n",
    "        Args:\n",
    "            filepath (Union[str, Path]): Path to save the history file\n",
    "        \"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        with filepath.open('w') as f:\n",
    "            json.dump(self.history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prompt engineer\n",
    "engineer = LocalPromptEngineer(model=\"llama3.3\")\n",
    "\n",
    "# Define a prompt structure for an educational task\n",
    "task = \"Create an explanation of binary search algorithms\"\n",
    "context = \"The explanation should be suitable for beginners and use inclusive language\"\n",
    "constraints = [\n",
    "    \"Use gender-neutral language\",\n",
    "    \"Avoid technical jargon without explanation\",\n",
    "    \"Include real-world examples\",\n",
    "    \"Consider diverse learning styles\"\n",
    "]\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"What is a loop?\",\n",
    "        \"output\": \"A loop is like a repeated action, similar to how you might check each drawer in a desk until you find what you're looking for.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the structured prompt\n",
    "prompt = engineer.create_structured_prompt(\n",
    "    task=task,\n",
    "    context=context,\n",
    "    constraints=constraints,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "# Generate a response (model is already specified during initialization)\n",
    "response = engineer.generate_response(\n",
    "    prompt=prompt,    # The structured prompt we created\n",
    "    temperature=0.7   # Controls response creativity (0.0 - 1.0)\n",
    ")\n",
    "\n",
    "# Evaluate the response\n",
    "evaluation_criteria = [\n",
    "    \"clarity\",\n",
    "    \"bias\",\n",
    "    \"technical_accuracy\",\n",
    "    \"engagement\"\n",
    "]\n",
    "scores = engineer.evaluate_response(response, evaluation_criteria)\n",
    "\n",
    "# Save the interaction history\n",
    "engineer.save_history(\"prompt_engineering_history.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Prompt Engineering\n",
    "1. Clarity and Structure\n",
    "\n",
    "- Use clear, specific instructions\n",
    "- Break down complex tasks into smaller components\n",
    "- Provide context and constraints explicitly\n",
    "\n",
    "2. Inclusivity and Neutrality\n",
    "\n",
    "- Use gender-neutral language\n",
    "- Consider diverse perspectives and experiences\n",
    "- Avoid cultural assumptions\n",
    "- Use accessible examples\n",
    "\n",
    "3. Technical Considerations\n",
    "\n",
    "- Specify output format requirements\n",
    "- Include error handling expectations\n",
    "- Define success criteria\n",
    "- Consider edge cases\n",
    "\n",
    "4. Response Evaluation\n",
    "\n",
    "- Define clear evaluation metrics\n",
    "- Check for bias in responses\n",
    "- Validate technical accuracy\n",
    "- Ensure accessibility of explanations\n",
    "\n",
    "# Common Pitfalls to Avoid\n",
    "\n",
    "1. Ambiguous instructions\n",
    "2. Implicit assumptions\n",
    "3. Lack of context\n",
    "4. Overly complex prompts\n",
    "5. Insufficient constraints\n",
    "6. Missing evaluation criteria\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "- Experiment with different prompt structures\n",
    "- Test with various models\n",
    "- Gather feedback from diverse users\n",
    "- Iterate based on evaluation results\n",
    "- Document successful patterns\n",
    "- Build a prompt template library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f57ffd79f07881f90ffd84d1ee449596c2bc3e88fee236dc006178dc960802e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
