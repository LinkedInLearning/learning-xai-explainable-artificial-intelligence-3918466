{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual Fairness Analysis for Generative AI Outputs\n",
    "\n",
    "This notebook implements tools for analyzing and optimizing fairness in generative AI\n",
    "outputs using counterfactual analysis. It helps identify potential biases by examining\n",
    "how outputs change when protected attributes are varied while holding other factors constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "### Prerequisites\n",
    "Ensure you have Python 3.8+ installed. The project dependencies are listed in requirements.txt:\n",
    "```bash \n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CounterfactualConfig:\n",
    "    \"\"\"Configuration for counterfactual generation and analysis.\"\"\"\n",
    "    protected_attributes: List[str]\n",
    "    similarity_threshold: float = 0.8\n",
    "    num_counterfactuals: int = 5\n",
    "    random_seed: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterfactualGenerator:\n",
    "    \"\"\"Generates counterfactual examples for fairness analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: CounterfactualConfig):\n",
    "        \"\"\"\n",
    "        Initialize the counterfactual generator.\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration object containing parameters\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        np.random.seed(config.random_seed)\n",
    "        \n",
    "    def generate_counterfactuals(\n",
    "        self,\n",
    "        original_input: Dict,\n",
    "        embeddings: np.ndarray\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate counterfactual examples by varying protected attributes.\n",
    "        \n",
    "        Args:\n",
    "            original_input: Original input features\n",
    "            embeddings: Pre-computed embeddings for similarity calculation\n",
    "            \n",
    "        Returns:\n",
    "            List of counterfactual examples\n",
    "        \"\"\"\n",
    "        counterfactuals = []\n",
    "        \n",
    "        for attr in self.config.protected_attributes:\n",
    "            # Create variations of protected attributes\n",
    "            variations = self._generate_attribute_variations(\n",
    "                original_input[attr],\n",
    "                attr\n",
    "            )\n",
    "            \n",
    "            for variation in variations:\n",
    "                counterfactual = original_input.copy()\n",
    "                counterfactual[attr] = variation\n",
    "                \n",
    "                # Only keep counterfactuals that are sufficiently similar\n",
    "                if self._check_similarity(\n",
    "                    original_input,\n",
    "                    counterfactual,\n",
    "                    embeddings\n",
    "                ):\n",
    "                    counterfactuals.append(counterfactual)\n",
    "                    \n",
    "        return counterfactuals[:self.config.num_counterfactuals]\n",
    "    \n",
    "    def _generate_attribute_variations(\n",
    "        self,\n",
    "        original_value: str,\n",
    "        attribute: str\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate variations for a protected attribute.\"\"\"\n",
    "        # This is a simplified example - in practice, you would want a more\n",
    "        # comprehensive approach to generating meaningful variations\n",
    "        if attribute == \"gender\":\n",
    "            return [\"male\", \"female\", \"non-binary\"]\n",
    "        elif attribute == \"age\":\n",
    "            return [\"young\", \"middle-aged\", \"senior\"]\n",
    "        elif attribute == \"ethnicity\":\n",
    "            return [\"Asian\", \"Black\", \"Hispanic\", \"White\", \"Other\"]\n",
    "        return [original_value]\n",
    "    \n",
    "    def _check_similarity(\n",
    "        self,\n",
    "        original: Dict,\n",
    "        counterfactual: Dict,\n",
    "        embeddings: np.ndarray\n",
    "    ) -> bool:\n",
    "        \"\"\"Check if counterfactual is sufficiently similar to original.\"\"\"\n",
    "        # Convert inputs to feature vectors (simplified)\n",
    "        orig_vec = self._dict_to_vector(original)\n",
    "        cf_vec = self._dict_to_vector(counterfactual)\n",
    "        \n",
    "        similarity = cosine_similarity(\n",
    "            orig_vec.reshape(1, -1),\n",
    "            cf_vec.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return similarity >= self.config.similarity_threshold\n",
    "    \n",
    "    def _dict_to_vector(self, input_dict: Dict) -> np.ndarray:\n",
    "        \"\"\"Convert input dictionary to feature vector.\"\"\"\n",
    "        # This is a simplified conversion - you would need to implement\n",
    "        # proper feature engineering in practice\n",
    "        return np.array([hash(str(v)) for v in input_dict.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessAnalyzer:\n",
    "    \"\"\"Analyzes fairness metrics across counterfactuals.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the fairness analyzer.\"\"\"\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def analyze_outputs(\n",
    "        self,\n",
    "        original_output: str,\n",
    "        counterfactual_outputs: List[str],\n",
    "        protected_attributes: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate fairness metrics across original and counterfactual outputs.\n",
    "        \n",
    "        Args:\n",
    "            original_output: Output from original input\n",
    "            counterfactual_outputs: Outputs from counterfactual inputs\n",
    "            protected_attributes: List of protected attributes\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of fairness metrics\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'output_similarity': self._calculate_output_similarity(\n",
    "                original_output,\n",
    "                counterfactual_outputs\n",
    "            ),\n",
    "            'attribute_impact': self._calculate_attribute_impact(\n",
    "                counterfactual_outputs,\n",
    "                protected_attributes\n",
    "            ),\n",
    "            'bias_score': self._calculate_bias_score(\n",
    "                original_output,\n",
    "                counterfactual_outputs\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        self.metrics = metrics\n",
    "        return metrics\n",
    "    \n",
    "    def _calculate_output_similarity(\n",
    "        self,\n",
    "        original: str,\n",
    "        counterfactuals: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate average similarity between original and counterfactual outputs.\"\"\"\n",
    "        # This is a simplified similarity calculation\n",
    "        # In practice, you might want to use more sophisticated NLP metrics\n",
    "        similarities = []\n",
    "        for cf in counterfactuals:\n",
    "            common_words = set(original.split()) & set(cf.split())\n",
    "            total_words = set(original.split()) | set(cf.split())\n",
    "            similarities.append(len(common_words) / len(total_words))\n",
    "        return np.mean(similarities)\n",
    "    \n",
    "    def _calculate_attribute_impact(\n",
    "        self,\n",
    "        outputs: List[str],\n",
    "        attributes: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Calculate impact score for each protected attribute.\"\"\"\n",
    "        impact_scores = {}\n",
    "        for attr in attributes:\n",
    "            # Simplified impact calculation\n",
    "            # In practice, you would want to use more sophisticated methods\n",
    "            attr_mentions = sum(1 for output in outputs if attr.lower() in output.lower())\n",
    "            impact_scores[attr] = attr_mentions / len(outputs)\n",
    "        return impact_scores\n",
    "    \n",
    "    def _calculate_bias_score(\n",
    "        self,\n",
    "        original: str,\n",
    "        counterfactuals: List[str]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate overall bias score based on output variations.\"\"\"\n",
    "        # Simplified bias score calculation\n",
    "        # Higher score indicates more potential bias\n",
    "        word_variations = len(set(\n",
    "            word for output in counterfactuals\n",
    "            for word in output.split()\n",
    "        )) - len(set(original.split()))\n",
    "        return word_variations / len(counterfactuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairnessVisualizer:\n",
    "    \"\"\"Visualizes fairness analysis results.\"\"\"\n",
    "    \n",
    "    def plot_metrics(self, metrics: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Create visualization of fairness metrics.\n",
    "        \n",
    "        Args:\n",
    "            metrics: Dictionary of calculated fairness metrics\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Output Similarity\n",
    "        plt.subplot(131)\n",
    "        plt.bar(['Output Similarity'], [metrics['output_similarity']])\n",
    "        plt.title('Output Similarity')\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Plot 2: Attribute Impact\n",
    "        plt.subplot(132)\n",
    "        impact_scores = metrics['attribute_impact']\n",
    "        plt.bar(impact_scores.keys(), impact_scores.values())\n",
    "        plt.title('Attribute Impact')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1)\n",
    "        \n",
    "        # Plot 3: Bias Score\n",
    "        plt.subplot(133)\n",
    "        plt.bar(['Bias Score'], [metrics['bias_score']])\n",
    "        plt.title('Bias Score')\n",
    "        plt.ylim(0, max(2, metrics['bias_score'] * 1.2))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_fairness(\n",
    "    generator: CounterfactualGenerator,\n",
    "    analyzer: FairnessAnalyzer,\n",
    "    original_input: Dict,\n",
    "    original_output: str,\n",
    "    embeddings: np.ndarray\n",
    ") -> Tuple[Dict, List[str]]:\n",
    "    \"\"\"\n",
    "    Optimize output fairness using counterfactual analysis.\n",
    "    \n",
    "    Args:\n",
    "        generator: Counterfactual generator instance\n",
    "        analyzer: Fairness analyzer instance\n",
    "        original_input: Original input features\n",
    "        original_output: Original model output\n",
    "        embeddings: Pre-computed embeddings\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of optimization suggestions and warnings\n",
    "    \"\"\"\n",
    "    counterfactuals = generator.generate_counterfactuals(original_input, embeddings)\n",
    "    metrics = analyzer.analyze_outputs(\n",
    "        original_output,\n",
    "        [cf['output'] for cf in counterfactuals],\n",
    "        generator.config.protected_attributes\n",
    "    )\n",
    "    \n",
    "    suggestions = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Analyze metrics and generate suggestions\n",
    "    if metrics['output_similarity'] < 0.7:\n",
    "        warnings.append(\n",
    "            \"High output variation detected across protected attributes\"\n",
    "        )\n",
    "        suggestions.append(\n",
    "            \"Consider adjusting model to maintain more consistent outputs\"\n",
    "        )\n",
    "    \n",
    "    for attr, impact in metrics['attribute_impact'].items():\n",
    "        if impact > 0.3:\n",
    "            warnings.append(\n",
    "                f\"High impact detected for attribute: {attr}\"\n",
    "            )\n",
    "            suggestions.append(\n",
    "                f\"Review handling of {attr} in model logic\"\n",
    "            )\n",
    "    \n",
    "    if metrics['bias_score'] > 0.5:\n",
    "        warnings.append(\"Significant potential bias detected\")\n",
    "        suggestions.append(\n",
    "            \"Consider retraining model with more balanced dataset\"\n",
    "        )\n",
    "    \n",
    "    return suggestions, warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = CounterfactualConfig(\n",
    "    protected_attributes=[\"gender\", \"age\", \"ethnicity\"],\n",
    "    similarity_threshold=0.8,\n",
    "    num_counterfactuals=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample input and output\n",
    "original_input = {\n",
    "    \"gender\": \"female\",\n",
    "    \"age\": \"young\",\n",
    "    \"ethnicity\": \"Asian\",\n",
    "    \"occupation\": \"engineer\",\n",
    "    \"education\": \"graduate\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_output = \"A talented young female engineer with graduate education\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.random.rand(10, 128)  # In practice, use real embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "generator = CounterfactualGenerator(config)\n",
    "analyzer = FairnessAnalyzer()\n",
    "visualizer = FairnessVisualizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and analyze counterfactuals\n",
    "counterfactuals = generator.generate_counterfactuals(original_input, embeddings)\n",
    "metrics = analyzer.analyze_outputs(\n",
    "    original_output,\n",
    "    [\n",
    "        \"A talented engineer with graduate education\",\n",
    "        \"A skilled professional in engineering\",\n",
    "        \"An experienced engineer with advanced degree\",\n",
    "        \"A qualified technical professional\",\n",
    "        \"A dedicated engineering specialist\"\n",
    "    ],\n",
    "    config.protected_attributes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "visualizer.plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get optimization suggestions\n",
    "suggestions, warnings = optimize_fairness(\n",
    "    generator,\n",
    "    analyzer,\n",
    "    original_input,\n",
    "    original_output,\n",
    "    embeddings\n",
    ")\n",
    "    \n",
    "print(\"\\nOptimization Suggestions:\")\n",
    "for suggestion in suggestions:\n",
    "    print(f\"- {suggestion}\")\n",
    "    \n",
    "print(\"\\nWarnings:\")\n",
    "for warning in warnings:\n",
    "    print(f\"- {warning}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57ffd79f07881f90ffd84d1ee449596c2bc3e88fee236dc006178dc960802e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
