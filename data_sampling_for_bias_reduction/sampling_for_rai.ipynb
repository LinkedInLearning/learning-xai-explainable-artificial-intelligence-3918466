{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling for Responsible AI Principles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates responsible sampling techniques for evaluating Large Language Models (LLMs)\n",
    "with a focus on bias mitigation and ethical considerations. We'll cover:\n",
    "\n",
    "1. Why balanced sampling matters for AI fairness\n",
    "2. Implementation of different sampling techniques\n",
    "3. Evaluation across diverse demographic groups\n",
    "4. Analysis of model performance disparities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "### Prerequisites\n",
    "Ensure you have Python 3.8+ installed. The project dependencies are listed in requirements.txt:\n",
    "```bash \n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(['stopwords', 'punkt_tab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponsibleDataSampler:\n",
    "    \"\"\"\n",
    "    A class to handle responsible data sampling and analysis for NLP tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_seed: int = 42):\n",
    "        \"\"\"\n",
    "        Initialize the sampler with optional random seed.\n",
    "        \n",
    "        Args:\n",
    "            random_seed: Seed for reproducibility\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        self.sentiment_analyzer = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            top_k=None\n",
    "        )\n",
    "        \n",
    "    def create_synthetic_dataset(\n",
    "        self,\n",
    "        n_samples: int = 1000,\n",
    "        weighted: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a synthetic dataset with demographic weights.\n",
    "        \n",
    "        Args:\n",
    "            n_samples: Number of samples to generate\n",
    "            weighted: Whether to apply demographic weights\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Synthetic dataset\n",
    "        \"\"\"\n",
    "        # Define demographics with realistic weights\n",
    "        demographics = {\n",
    "            'gender': {\n",
    "                'categories': ['female', 'male', 'non_binary'],\n",
    "                'weights': [0.49, 0.48, 0.03] if weighted else None\n",
    "            },\n",
    "            'age_group': {\n",
    "                'categories': ['18-25', '26-40', '41-60', '60+'],\n",
    "                'weights': [0.2, 0.35, 0.3, 0.15] if weighted else None\n",
    "            },\n",
    "            'region': {\n",
    "                'categories': ['north', 'south', 'east', 'west'],\n",
    "                'weights': [0.3, 0.35, 0.15, 0.2] if weighted else None\n",
    "            },\n",
    "            'language': {\n",
    "                'categories': ['english', 'spanish', 'mandarin', 'arabic'],\n",
    "                'weights': [0.45, 0.25, 0.2, 0.1] if weighted else None\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Generate demographic data\n",
    "        df_dict = {}\n",
    "        for feature, data in demographics.items():\n",
    "            df_dict[feature] = np.random.choice(\n",
    "                data['categories'],\n",
    "                n_samples,\n",
    "                p=data['weights'] if weighted else None\n",
    "            )\n",
    "            \n",
    "        # Generate synthetic text with demographic-specific patterns\n",
    "        texts = []\n",
    "        for i in range(n_samples):\n",
    "            sentiment = np.random.choice(['positive', 'negative'])\n",
    "            if sentiment == 'positive':\n",
    "                template = np.random.choice([\n",
    "                    \"I really enjoyed this product!\",\n",
    "                    \"The service was excellent.\",\n",
    "                    \"This helps me a lot.\",\n",
    "                ])\n",
    "            else:\n",
    "                template = np.random.choice([\n",
    "                    \"I'm disappointed with this product.\",\n",
    "                    \"The service was poor.\",\n",
    "                    \"This doesn't work for me.\",\n",
    "                ])\n",
    "            texts.append(template)\n",
    "        \n",
    "        df_dict['text'] = texts\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(df_dict)\n",
    "        \n",
    "        # Add sentiment labels with demographic bias\n",
    "        df['label'] = np.where(\n",
    "            (df['age_group'] == '18-25') & (df['gender'] == 'female'),\n",
    "            np.random.choice(['positive', 'negative'], n_samples, p=[0.7, 0.3]),\n",
    "            np.random.choice(['positive', 'negative'], n_samples, p=[0.5, 0.5])\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def analyze_text_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Analyzes text features across demographic groups.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with text analysis features\n",
    "        \"\"\"\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        def get_text_metrics(text: str) -> Dict[str, float]:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            tokens_no_stop = [t for t in tokens if t not in stop_words]\n",
    "            \n",
    "            return {\n",
    "                'text_length': len(text),\n",
    "                'word_count': len(tokens),\n",
    "                'vocabulary_diversity': len(set(tokens_no_stop)) / len(tokens_no_stop) if tokens_no_stop else 0\n",
    "            }\n",
    "        \n",
    "        # Calculate text metrics\n",
    "        text_metrics = df['text'].apply(get_text_metrics).apply(pd.Series)\n",
    "        df = pd.concat([df, text_metrics], axis=1)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def perform_stratified_sampling(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        strat_cols: List[str],\n",
    "        sample_size: int\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Performs stratified sampling across specified columns.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            strat_cols: Columns to stratify on\n",
    "            sample_size: Desired sample size\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Stratified sample\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df['strata'] = df[strat_cols].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "        \n",
    "        # Calculate sample sizes for each stratum\n",
    "        stratum_sizes = (df['strata'].value_counts(normalize=True) * sample_size).round().astype(int)\n",
    "        \n",
    "        # Sample from each stratum\n",
    "        samples = []\n",
    "        for stratum, size in stratum_sizes.items():\n",
    "            stratum_df = df[df['strata'] == stratum]\n",
    "            if len(stratum_df) < size:\n",
    "                samples.append(stratum_df)  # Take all if not enough samples\n",
    "            else:\n",
    "                samples.append(stratum_df.sample(n=size))\n",
    "        \n",
    "        return pd.concat(samples).drop('strata', axis=1)\n",
    "    \n",
    "    def analyze_model_biases(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        demographic_cols: List[str]\n",
    "    ) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyzes model predictions across demographic groups.\n",
    "    \n",
    "        Args:\n",
    "            df: DataFrame with text data\n",
    "            demographic_cols: Demographic columns to analyze\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Bias analysis results\n",
    "        \"\"\"\n",
    "        bias_metrics = {}\n",
    "    \n",
    "        for col in demographic_cols:\n",
    "            bias_metrics[col] = {}\n",
    "        \n",
    "            for group in df[col].unique():\n",
    "                group_texts = df[df[col] == group]['text'].tolist()\n",
    "            \n",
    "                # Get model predictions\n",
    "                predictions = self.sentiment_analyzer(group_texts)\n",
    "            \n",
    "                # Handle different output formats from the pipeline\n",
    "                if isinstance(predictions, list) and isinstance(predictions[0], list):\n",
    "                    # When top_k=None, format is list of lists\n",
    "                    pred_labels = [p[0]['label'] for p in predictions]\n",
    "                    confidences = [p[0]['score'] for p in predictions]\n",
    "                else:\n",
    "                    # Standard format when top_k is not specified\n",
    "                    pred_labels = [p['label'] for p in predictions]\n",
    "                    confidences = [p['score'] for p in predictions]\n",
    "            \n",
    "                # Calculate metrics\n",
    "                bias_metrics[col][group] = {\n",
    "                    'representation': len(group_texts) / len(df),\n",
    "                    'avg_confidence': np.mean(confidences),\n",
    "                    'pred_distribution': Counter(pred_labels)\n",
    "                }\n",
    "    \n",
    "        return bias_metrics\n",
    "    \n",
    "    def plot_demographic_distributions(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        demographic_cols: List[str]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plots the distribution of samples across demographic groups.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            demographic_cols: Demographic columns to plot\n",
    "        \"\"\"\n",
    "        n_cols = len(demographic_cols)\n",
    "        fig, axes = plt.subplots(1, n_cols, figsize=(5*n_cols, 4))\n",
    "        \n",
    "        for i, col in enumerate(demographic_cols):\n",
    "            sns.countplot(data=df, x=col, ax=axes[i])\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_bias_metrics(self, bias_metrics: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Visualizes bias metrics across demographic groups.\n",
    "        \n",
    "        Args:\n",
    "            bias_metrics: Dictionary of bias metrics\n",
    "        \"\"\"\n",
    "        n_metrics = len(bias_metrics)\n",
    "        fig, axes = plt.subplots(1, n_metrics, figsize=(6*n_metrics, 4))\n",
    "        \n",
    "        for i, (demographic, group_metrics) in enumerate(bias_metrics.items()):\n",
    "            groups = list(group_metrics.keys())\n",
    "            confidences = [m['avg_confidence'] for m in group_metrics.values()]\n",
    "            \n",
    "            sns.barplot(x=groups, y=confidences, ax=axes[i])\n",
    "            axes[i].set_title(f'{demographic} - Avg Confidence')\n",
    "            axes[i].set_ylim(0, 1)\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sampler\n",
    "sampler = ResponsibleDataSampler()\n",
    "    \n",
    "# Create and analyze dataset\n",
    "df = sampler.create_synthetic_dataset(n_samples=500, weighted=True)\n",
    "df = sampler.analyze_text_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform stratified sampling\n",
    "demographic_cols = ['gender', 'age_group', 'region', 'language']\n",
    "sample_size = 500\n",
    "stratified_sample = sampler.perform_stratified_sampling(\n",
    "    df,\n",
    "    demographic_cols,\n",
    "    sample_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze biases\n",
    "bias_metrics = sampler.analyze_model_biases(\n",
    "    stratified_sample,\n",
    "    demographic_cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(df.head())\n",
    "    \n",
    "print(\"\\n=== Distribution of Demographics ===\")\n",
    "sampler.plot_demographic_distributions(df, demographic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Bias Analysis ===\")\n",
    "sampler.plot_bias_metrics(bias_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things to Remember \n",
    "\n",
    "\n",
    "1. Representative Sampling\n",
    "   - Ensures all demographic groups are adequately represented\n",
    "   - Helps identify and address performance disparities\n",
    "   - Critical for building inclusive AI systems\n",
    "\n",
    "2. Stratified Sampling Benefits\n",
    "   - Maintains population proportions\n",
    "   - Reduces sampling bias\n",
    "   - Enables meaningful comparison across groups\n",
    "\n",
    "3. Fairness Evaluation\n",
    "   - Regular monitoring of performance across groups\n",
    "   - Identification of potential biases\n",
    "   - Guide for model improvements\n",
    "\n",
    "4. Ethical Considerations\n",
    "   - Privacy protection in demographic data collection\n",
    "   - Transparent reporting of limitations\n",
    "   - Regular updates to sampling strategies\n",
    "\n",
    "5. Best Practices\n",
    "   - Document sampling methodology\n",
    "   - Version control for reproducibility\n",
    "   - Regular audits of sampling process\n",
    "   - Consultation with domain experts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f57ffd79f07881f90ffd84d1ee449596c2bc3e88fee236dc006178dc960802e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
